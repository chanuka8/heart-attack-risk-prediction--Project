# -*- coding: utf-8 -*-
"""Assignment 01 FINAL BI .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jhmM6f-KUidV7fz4UWQ1vpiQvOnvPH_q
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset (Replace with your file path)
from google.colab import files
uploaded = files.upload()  # Upload your dataset file

import pandas as pd
df = pd.read_csv("heart_attack_risk.csv")

num_rows = df.shape[0]
print(f"Total number of rows: {num_rows}")

df.head()

print(df.dtypes)  # Displays the data types of each column

# Check for missing values
print(df.isnull().sum())

# fill missing values with the mean value
df.fillna(df.mean(), inplace=True)

sns.set_style("whitegrid")

#histograms for numerical features
df.hist(figsize=(12, 8), bins=15, edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

#heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

#boxplot
sns.boxplot(x='sex', y='total_cholesterol', data=df)
plt.title('Total Cholesterol by Sex')
plt.show()

#bar plots
sns.countplot(x='sex', data=df)
plt.title('Count of Individuals by Sex')
plt.show()

#stacked bar plot
pd.crosstab(df['smoking'], df['heart_attack']).plot(kind='bar', stacked=True)
plt.title('Smoking and Heart Attack Relationship')
plt.show()

# Define features (X) and target (y)
X = df.drop("heart_attack", axis=1)  # Features
y = df["heart_attack"]  # Target

# Standardizing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Spliting into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initializing the model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)



# Evaluate model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score

# Initialize the Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Predict probability estimates for RÂ² calculation
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Taking probability of class 1

r2 = r2_score(y_test, y_pred_prob)

# Print evaluation metrics
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nRÂ² Score:", r2)

#logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score

# Initialize model
log_reg = LogisticRegression()

# Train
log_reg.fit(X_train, y_train)

# predictions
y_pred_log = log_reg.predict(X_test)

# Compute
y_pred_prob_log = log_reg.predict_proba(X_test)[:, 1]

# Compute RÂ²
r2_log = r2_score(y_test, y_pred_prob_log)

# Print results
print("ðŸ”¹ Logistic Regression Results ðŸ”¹")
print("Accuracy Score:", accuracy_score(y_test, y_pred_log))
print("\nClassification Report:\n", classification_report(y_test, y_pred_log))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_log))
print("\nRÂ² Score:", r2_log)

#decision tree classifier
from sklearn.tree import DecisionTreeClassifier

# Initializing model
dt = DecisionTreeClassifier(random_state=42)

# Train
dt.fit(X_train, y_train)

# predictions
y_pred_dt = dt.predict(X_test)

# Compute
y_pred_prob_dt = dt.predict_proba(X_test)[:, 1]

# Compute RÂ²
r2_dt = r2_score(y_test, y_pred_prob_dt)

# Print results
print("ðŸ”¹ Decision Tree Results ðŸ”¹")
print("Accuracy Score:", accuracy_score(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("\nRÂ² Score:", r2_dt)

#xgboost
from xgboost import XGBClassifier

# Initializing
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Train
xgb.fit(X_train, y_train)

# predictions
y_pred_xgb = xgb.predict(X_test)

# Compute
y_pred_prob_xgb = xgb.predict_proba(X_test)[:, 1]

# Compute RÂ²
r2_xgb = r2_score(y_test, y_pred_prob_xgb)

# Print results
print("ðŸ”¹ XGBoost Results ðŸ”¹")
print("Accuracy Score:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nRÂ² Score:", r2_xgb)

